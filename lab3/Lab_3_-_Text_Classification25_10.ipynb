{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Text Classification, NLTK, Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ревізії"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:03.316886100Z",
     "start_time": "2023-11-20T22:12:01.165843400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           Date                                            Comment\n0  Sep 20, 2019                                   Початкова версія\n1  Oct 14, 2019                    Виправлено орфографічні помилки\n2  Oct 20, 2019  Додано сортування текстів в датасетах. Виправл...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sep 20, 2019</td>\n      <td>Початкова версія</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Oct 14, 2019</td>\n      <td>Виправлено орфографічні помилки</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Oct 20, 2019</td>\n      <td>Додано сортування текстів в датасетах. Виправл...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "revisions = pd.DataFrame(\n",
    "    columns=[\"Date\", \"Comment\"]\n",
    ")\n",
    "revisions = pd.concat([\n",
    "    revisions,\n",
    "    pd.DataFrame([{\n",
    "        \"Date\": \"Sep 20, 2019\",\n",
    "        \"Comment\": \"Початкова версія\"\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "revisions = pd.concat([\n",
    "    revisions,\n",
    "    pd.DataFrame([{\n",
    "        \"Date\": \"Oct 14, 2019\",\n",
    "        \"Comment\": \"Виправлено орфографічні помилки\"\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "revisions = pd.concat([\n",
    "    revisions,\n",
    "    pd.DataFrame([{\n",
    "        \"Date\": \"Oct 20, 2019\",\n",
    "        \"Comment\": \"Додано сортування текстів в датасетах. Виправлено проблему TF/IDF = 0\"\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "revisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Використовуючи бібліотеку NLTK та алгоритм NaiveBayes побудувати модель визначення авторства текстів, написаних англійською мовою."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Підготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переконайтесь, що у вас встановлено пакети, необхідні для виконання роботи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:27.975546400Z",
     "start_time": "2023-11-20T22:12:01.851192100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обробки тексту ми використовуватимемо бібліотеку NLTK. Зробимо необхідні імпорти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:29.911103500Z",
     "start_time": "2023-11-20T22:12:27.975546400Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфігуруємо бібліотеки під потреби даної задачі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:29.942210500Z",
     "start_time": "2023-11-20T22:12:29.915622900Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is a directory where supporting files for NLTK library are located.\n",
    "# This files include dictionaries for different languages, prebuilt models etc.\n",
    "NLTK_DATA_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "nltk.data.path.insert(0, NLTK_DATA_DIR)\n",
    "\n",
    "# Inlines Matplotlib graphs in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Let's also enlarge the maximum width of the pandas column.\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для роботи NLTK необхідно завантажити деякі моделі та набори даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:30.175966100Z",
     "start_time": "2023-11-20T22:12:29.928078600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Admin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizers\n",
    "nltk.download(\"punkt\", download_dir=NLTK_DATA_DIR)\n",
    "nltk.download(\"wordnet\", download_dir=NLTK_DATA_DIR)\n",
    "nltk.download(\"stopwords\", download_dir=NLTK_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Завантаження даних та формування простору ознак"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разом з цією практичною ви завантажили набір творів англійською мовою. У вас є два набори даних. Перший навчальний (папка `train`), та другий тестовий (папка `test`). Усі твори погруповано по папках, де назва папки - це ім'я автора творів, що знаходяться в цій папці. Усі твори збережені в текстових файлах з розширенням файлу `txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:30.175966100Z",
     "start_time": "2023-11-20T22:12:30.160999Z"
    }
   },
   "outputs": [],
   "source": [
    "files = [(Path(f).parent.name, f) for f in glob.glob(\"train/**/*.txt\", recursive=True)]\n",
    "# Sort the files by relative file path\n",
    "files.sort(key=lambda tup: tup[1])\n",
    "for author, file in files:\n",
    "    print(author, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завантажте дані з файлів та створіть `pandas.DataFrame` з колонками (`Author`, `Text`), де `Author` встановіть за назвою директорії, а `Text` - вміст файлу. Переконайтесь, що дані посортовано но відносному шляху до файлу (дивіться приклад вище)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:30.187405400Z",
     "start_time": "2023-11-20T22:12:30.171260Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    data_dir_path = os.sep.join(['data', data_dir])\n",
    "    authors_info = [dir_info[0].replace(os.sep.join([data_dir_path, \"\"]), '') for dir_info in os.walk(data_dir_path)][\n",
    "                   1:]\n",
    "    texts_info = [os.listdir(os.sep.join([data_dir_path, path, \"\"])) for path in authors_info]\n",
    "\n",
    "    authors = []\n",
    "    texts = []\n",
    "\n",
    "    # Write your code here\n",
    "    for author_texts in texts_info:\n",
    "        author = authors_info[texts_info.index(author_texts)]\n",
    "\n",
    "        authors += [author] * len(author_texts)\n",
    "        texts += map(lambda path: Path(os.sep.join([data_dir_path, author, path])).read_text(), author_texts)\n",
    "    # Make sure you have items sorted by relative file path\n",
    "\n",
    "    return pd.DataFrame({'Author': authors, 'Text': texts})\n",
    "\n",
    "\n",
    "def fix_data_types(df):\n",
    "    df['Author'] = df.Author.astype(str)\n",
    "    df['Text'] = df.Text.astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    return fix_data_types(load_data('train'))\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    return fix_data_types(load_data('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:41:26.450403800Z",
     "start_time": "2023-11-20T22:41:25.550462600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Author  2500 non-null   object\n",
      " 1   Text    2500 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 39.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": "          Author  \\\n0  AaronPressman   \n1  AaronPressman   \n2  AaronPressman   \n3  AaronPressman   \n4  AaronPressman   \n5  AaronPressman   \n6  AaronPressman   \n7  AaronPressman   \n8  AaronPressman   \n9  AaronPressman   \n\n                                                                                                                                                    Text  \n0  The Internet may be overflowing with new technology but crime in cyberspace is still of the old-fashioned variety.\\nThe National Consumers League ...  \n1  The U.S. Postal Service announced Wednesday a plan to boost online commerce by enhancing the security and reliability of electronic mail traveling...  \n2  Elementary school students with access to the Internet learned more than kids who lacked access, an indepedent research group concluded after cond...  \n3  An influential Internet organisation has backed away from a proposal to dramatically expand the number of addresses available on the global comput...  \n4  An influential Internet organisation has backed away from a proposal to dramatically expand the number of addresses available on the global comput...  \n5  A group of leading trademark specialists plans to release recommendations aimed at minimizing disputes over Internet address names.\\nThe Internati...  \n6  When a company in California sells a book to a consumer in Canada from a Web site hosted on a computer in England, what laws govern the transactio...  \n7  U.S. laws governing the trillion dollar futures markets could be rocked by the Supreme Court's interpretation of the word \"in\" in a case to be arg...  \n8  Supreme Court justices Wednesday sharply questioned rules governing so-called derivative investments and foreign currency trading in a case that c...  \n9  The Internet continued to grow in leaps and bounds this year while online services found it much harder to add new customers, a survey said Friday...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Author</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AaronPressman</td>\n      <td>The Internet may be overflowing with new technology but crime in cyberspace is still of the old-fashioned variety.\\nThe National Consumers League ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AaronPressman</td>\n      <td>The U.S. Postal Service announced Wednesday a plan to boost online commerce by enhancing the security and reliability of electronic mail traveling...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AaronPressman</td>\n      <td>Elementary school students with access to the Internet learned more than kids who lacked access, an indepedent research group concluded after cond...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AaronPressman</td>\n      <td>An influential Internet organisation has backed away from a proposal to dramatically expand the number of addresses available on the global comput...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AaronPressman</td>\n      <td>An influential Internet organisation has backed away from a proposal to dramatically expand the number of addresses available on the global comput...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>AaronPressman</td>\n      <td>A group of leading trademark specialists plans to release recommendations aimed at minimizing disputes over Internet address names.\\nThe Internati...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>AaronPressman</td>\n      <td>When a company in California sells a book to a consumer in Canada from a Web site hosted on a computer in England, what laws govern the transactio...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>AaronPressman</td>\n      <td>U.S. laws governing the trillion dollar futures markets could be rocked by the Supreme Court's interpretation of the word \"in\" in a case to be arg...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>AaronPressman</td>\n      <td>Supreme Court justices Wednesday sharply questioned rules governing so-called derivative investments and foreign currency trading in a case that c...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>AaronPressman</td>\n      <td>The Internet continued to grow in leaps and bounds this year while online services found it much harder to add new customers, a survey said Friday...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = load_test_data()\n",
    "train_df = load_train_data()\n",
    "train_df.info()\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обчисліть кількість творів для кожного автора в навчальному датасеті."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:30.852701Z",
     "start_time": "2023-11-20T22:12:30.818742900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                   Text\nAuthor                 \nAaronPressman        50\nAlanCrosby           50\nAlexanderSmith       50\nBenjaminKangLim      50\nBernardHickey        50\nBradDorfman          50\nDarrenSchuettler     50\nDavidLawder          50\nEdnaFernandes        50\nEricAuchard          50\nFumikoFujisaki       50\nGrahamEarnshaw       50\nHeatherScoffield     50\nJanLopatka           50\nJaneMacartney        50\nJimGilchrist         50\nJoWinterbottom       50\nJoeOrtiz             50\nJohnMastrini         50\nJonathanBirt         50\nKarlPenhaul          50\nKeithWeir            50\nKevinDrawbaugh       50\nKevinMorrison        50\nKirstinRidley        50\nKouroshKarimkhany    50\nLydiaZajc            50\nLynneO'Donnell       50\nLynnleyBrowning      50\nMarcelMichelson      50\nMarkBendeich         50\nMartinWolk           50\nMatthewBunce         50\nMichaelConnor        50\nMureDickie           50\nNickLouth            50\nPatriciaCommins      50\nPeterHumphrey        50\nPierreTran           50\nRobinSidel           50\nRogerFillion         50\nSamuelPerry          50\nSarahDavison         50\nScottHillis          50\nSimonCowell          50\nTanEeLyn             50\nTheresePoletti       50\nTimFarrand           50\nToddNissen           50\nWilliamKazer         50",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n    </tr>\n    <tr>\n      <th>Author</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AaronPressman</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>AlanCrosby</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>AlexanderSmith</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>BenjaminKangLim</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>BernardHickey</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>BradDorfman</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>DarrenSchuettler</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>DavidLawder</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>EdnaFernandes</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>EricAuchard</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>FumikoFujisaki</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>GrahamEarnshaw</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>HeatherScoffield</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>JanLopatka</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>JaneMacartney</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>JimGilchrist</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>JoWinterbottom</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>JoeOrtiz</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>JohnMastrini</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>JonathanBirt</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>KarlPenhaul</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>KeithWeir</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>KevinDrawbaugh</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>KevinMorrison</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>KirstinRidley</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>KouroshKarimkhany</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>LydiaZajc</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>LynneO'Donnell</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>LynnleyBrowning</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>MarcelMichelson</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>MarkBendeich</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>MartinWolk</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>MatthewBunce</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>MichaelConnor</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>MureDickie</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>NickLouth</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>PatriciaCommins</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>PeterHumphrey</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>PierreTran</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>RobinSidel</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>RogerFillion</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>SamuelPerry</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>SarahDavison</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>ScottHillis</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>SimonCowell</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>TanEeLyn</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>TheresePoletti</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>TimFarrand</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>ToddNissen</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>WilliamKazer</th>\n      <td>50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "train_df.groupby(\"Author\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Визначення**. Датасет називається `збалансованим` якщо кількість запитів в для кожної мітки є приблизно однаковою."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запитання**. Враховуючи що Автор в даному випадку є міткою датасету, дайте відповідь на запитання чи є навчальний датасет збалансованим?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Ваша відповідь**:</span> Датасет не є збалансованим, оскільки на 1 автора (мітка x) приходиться n (n > 1) творів (мітка y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Зауваження**. Якщо датасет збалансований, ви можете використовувати практично будь-який алгоритм машинного навчання. Якщо датасет розбалансований, вам необхідно його додатково обробляти (збалансувати), або використовувати алгоритми, що вміють працювати з незбалансованими датасетами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**a)**</span> Об'єднайте всі тексти з `train_df` у одну стрічку. Назвіть стрічку `combined_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:30.852701Z",
     "start_time": "2023-11-20T22:12:30.835578500Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_text = ' '.join(list(train_df['Text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**b)**</span> Напишіть функцію `normalize_text` та замініть у тексті знаки нового рядка (`\\n`) на пропуски (` `). Якщо ви вважаєте, що реєстр літер не важливий для даної задачі, то приведіть усі символи до одного реєстру. Також на власний розсуд замініть інші неважливі знаки у тексті."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:30.935657600Z",
     "start_time": "2023-11-20T22:12:30.845562400Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalizes the input string for the task of authorship verification\n",
    "    \"\"\"\n",
    "    normalized_text = re.sub(r'\\d+|\\\\p{Punct}', '', text.replace('\\n', ' ').lower()).strip()\n",
    "    # Write your code here\n",
    "\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**c)**</span> Використовуючи NLTK, напишіть функцію `tokenize_text`, яка поділить текст на окремі слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:30.970420500Z",
     "start_time": "2023-11-20T22:12:30.847851Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Splits the input string into a list of lexical units\n",
    "    \"\"\"\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    # Write your code here\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створюємо `Series` з слів тексту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:42.012518200Z",
     "start_time": "2023-11-20T22:12:30.855220500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0                  the\n1             internet\n2                  may\n3                   be\n4          overflowing\n              ...     \n1416404            for\n1416405            the\n1416406       congress\n1416407              .\n1416408             ''\nLength: 1416409, dtype: object"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.Series(tokenize_text(normalize_text(combined_text)))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**a)**</span> Скільки всього унікальних слів в `words`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:42.154980100Z",
     "start_time": "2023-11-20T22:12:42.008253200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "31343"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_count = pd.Series(words).nunique()  # Write your code here\n",
    "unique_words_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**b)**</span> Виведіть 30 найбільш вживаних слів? Як ви вважаєте, чи важливі ці слова при визначенні авторства? Чому?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "No influence is encountered cause these words are used in general grammatical constructions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:12:42.292151400Z",
     "start_time": "2023-11-20T22:12:42.279518500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "the     71352\n,       63452\n.       60300\nto      36280\nof      33921\na       29140\nin      27699\nand     25928\nsaid    19856\n's      14877\n``      13847\n''      13747\nfor     12249\non      12085\nthat    10478\nis       9923\nit       9536\nwith     7794\nbe       7518\n$        7466\nat       7283\nby       7215\nits      6888\nas       6874\nwas      6710\nfrom     6178\nhe       5976\nwill     5877\nbut      5622\nhas      5560\nName: count, dtype: int64"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "word_counts = pd.Series(words).value_counts()\n",
    "word_counts[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При аналізі тексту є ряд ключових показників оцінки важливості слів. Розглянемо їх в цьому розділі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частота слова (Term Frequency або скорочено TF) - відношення числа входжень обраного слова до загальної кількості слів документу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "df = \\frac{number\\;of\\;instances\\;of\\;the\\;word\\;in\\;the\\;document}{total\\;number\\;of\\;words\\;in\\;document}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 5</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обчисліть TF для слова `internet` в першому творі навчального датасету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def word_tf(word):\n",
    "    return word_counts[word] / word_counts.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T22:13:47.283884300Z",
     "start_time": "2023-11-20T22:13:47.279856700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:13:48.316025300Z",
     "start_time": "2023-11-20T22:13:48.268163300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term frequency for word 'internet' is 0.0006262315475261736\n"
     ]
    }
   ],
   "source": [
    "tf = word_tf('internet')\n",
    "# Write your code here\n",
    "print(\"Term frequency for word 'internet' is\", tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обернена частота документа (Inverse Document Frequency або скорочено IDF) - інверсія частоти, з якою слово зустрічається в документах колекції. Чим частіше слово зустрічається у всіх документах колекції тим нижчим буде його IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "idf = \\log(\\frac{number\\;of\\;documents\\;in\\;the\\;dataset}{number\\;of\\;documents\\;in\\;the\\;dataset\\;which\\;contain\\;given\\;word})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зверніть увагу, що основа логарифму не має значення."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 6</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обчисліть IDF для слів `deposit` та `business`. Використайте натуральний логарифм (основа `e`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def word_idf(word):\n",
    "    return math.log(len(train_df['Text']) / len(train_df[train_df['Text'].str.contains(word)]['Text']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T22:13:49.892251700Z",
     "start_time": "2023-11-20T22:13:49.876319100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:13:51.157143Z",
     "start_time": "2023-11-20T22:13:51.133895900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term frequency for work 'deposit' is 2.995732273553991\n",
      "Term frequency for work 'business' is 0.906340401020987\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idf_deposit = word_idf('deposit')\n",
    "idf_business = word_idf('business')\n",
    "# Write your code here\n",
    "print(\"Term frequency for work 'deposit' is\", idf_deposit)\n",
    "print(\"Term frequency for work 'business' is\", idf_business)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF визначається як добуток двох множників: TF та IDF. Таким чином слова з високою частотою появи в межах документа та низькою частотою вживання в інших документах отримують більшу вагу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 7</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обчисліть TF-IDF для слів `scam` та `money` в першому документі навчального датасету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def word_tf_idf(word):\n",
    "    return word_tf(word) * word_idf(word)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T22:13:52.902326Z",
     "start_time": "2023-11-20T22:13:52.895223900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:13:53.853830200Z",
     "start_time": "2023-11-20T22:13:53.834088600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(3.178077198617107e-05, 0.0006946770858845027)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "scam_tf_idf = word_tf_idf('scam')\n",
    "money_tf_idf = word_tf_idf('money')\n",
    "\n",
    "scam_tf_idf, money_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Побудова простої моделі"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На щастя, витягнення TF-IDF вже імплементовано за вас у бібліотеці `scikit-learn`. Для цього існує клас [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Зауваження.** Практично всі моделі в Scikit-Learn мають спільний інтуітивний інтерфейс. В конструкторі моделі ви передаєте набір параметрів для конфігурації моделі.Є дві групи моделей: моделі обробки та моделі класифікації.\n",
    "\n",
    "Моделі обробки даних зазвичай мають методи `fit_transform` та `transform`. Метод `fit_transform` використовується одночасно для навчання моделі на даних та їх перетворення, а метод `transform` виключно для перетворення даних (з використанням попередньо навченої моделі).\n",
    "\n",
    "Моделі класифікації даних зазвичай мають методи `fit` та `predict`. Метод `fit` передбачає навчання моделі, а метод `predict` - використання вже навченої моделі для отримання передбачень."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важливо**. Використовуйте методи `fit` та `fit_transform` виключно з навчальними даними. Для тестових даних використовуйте `tranform` та `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 8</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створіть об'єкт класу TfidfVectorizer. Сконфігуруйте його з наступними параметрами:\n",
    "- sublinear_tf=True\n",
    "- max_df=0.5\n",
    "- stopwords='english'\n",
    "\n",
    "Виконайте навчання та перетворення документів з навчального та тестового датасетів.\n",
    "Результат перетворення документів навчального датасету запишіть у `features_train_transformed`. Результат перетворення документів тестового датасету запишіть у `features_test_transformed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:15:31.697628800Z",
     "start_time": "2023-11-20T22:15:29.719430400Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    max_df=.5,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "features_train_transformed = vectorizer.fit_transform(train_df['Text'])\n",
    "features_test_transformed = vectorizer.transform(test_df['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Який розмір має матриця `features_train_transformed`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:15:36.574546700Z",
     "start_time": "2023-11-20T22:15:36.522363800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((2500, 28839), (2500, 28839))"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_transformed.shape, features_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як ви помітили, матриця має доволі багато характеристик документу. Це може спричинити проблеми при навчанні, тому давайте усунемо ряд характеристик, що мають найменший вплив на модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 9</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Використайте [`SelectPerceptile`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html) з функцією оцінки впливу `f_classif`. Встановіть значення `percentile` в 20%.\n",
    "\n",
    "Результат перетворення `features_train_transformed` запишіть у `features_train_transformed_optimized`. Результат перетворення `features_test_transformed` запишіть у `features_test_transformed_optimized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:16:35.841358600Z",
     "start_time": "2023-11-20T22:16:35.671300700Z"
    }
   },
   "outputs": [],
   "source": [
    "percentileTransformer = SelectPercentile(f_classif, percentile=20)\n",
    "features_train_transformed_optimized = percentileTransformer.fit_transform(features_train_transformed,\n",
    "                                                                           train_df['Author']).toarray()\n",
    "features_test_transformed_optimized = percentileTransformer.transform(features_test_transformed).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Який розмір має матриця `features_train_transformed_optimized`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:16:37.293775500Z",
     "start_time": "2023-11-20T22:16:37.286049500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((2500, 5768), (2500, 5768))"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_transformed_optimized.shape, features_test_transformed_optimized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основі отриманих характеристик будуємо модель. Використайте код нижче. Поки що ми не будемо вдаватись в деталі реалізації моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:16:44.483936200Z",
     "start_time": "2023-11-20T22:16:39.050993500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['AaronPressman', 'AaronPressman', 'TheresePoletti', ...,\n       'WilliamKazer', 'GrahamEarnshaw', 'BenjaminKangLim'], dtype='<U17')"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(features_train_transformed_optimized, train_df['Author'])\n",
    "predicted = clf.predict(features_test_transformed_optimized)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Оцінка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розглянемо ситуацію коли ми маємо дві моделі, які виконують однакову роботу (передбачають автора). Як порівняти ці моделі? Яка працює краще, а яка гірше? Відповідь на ці запитання можна отримати за допомогою методів оцінки моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним з базових методів оцінки моделі є точність (`accuracy`). Точність визначається кількість правильно класифікованих елементів певного датасету, до загальної кількості елементів в цьому датасеті."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:16:46.477486600Z",
     "start_time": "2023-11-20T22:16:46.463420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.5816"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_labels = test_df['Author']\n",
    "accuracy_score(correct_labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Іншими методами оцінки якості моделі є влучність (precision) та повнота (recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1872/1*pOtBHai4jFd-ujaNXPilRg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:16:48.838801700Z",
     "start_time": "2023-11-20T22:16:48.810933600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.6225548364039921"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(correct_labels, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:16:50.220699200Z",
     "start_time": "2023-11-20T22:16:50.201149300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.5816"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(correct_labels, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 10</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встановлюючи значення 10, 30, 40, 50 по черзі у поле `percentile` в `SelectPercentile` перегенеруйте навчальні та тестові дані. На основі нових даних виконайте навчання моделі та оцініть її якість. Як змінюється точність моделі зі зміною значення `perceptile` (більшою кількістю характеристик)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def get_gauss_nb(X, y, X_test):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X, y)\n",
    "    predicted = clf.predict(X_test)\n",
    "\n",
    "    return predicted\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T22:16:54.710804Z",
     "start_time": "2023-11-20T22:16:54.707286900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:19:48.670847800Z",
     "start_time": "2023-11-20T22:19:16.498541400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6102268932985803 0.5772\n",
      "0.6276242141550642 0.5844\n",
      "0.6377922887410735 0.5852\n",
      "0.6433692749714219 0.594\n"
     ]
    }
   ],
   "source": [
    "for percentile in (10, 30, 40, 50):\n",
    "    percentile_transformer = SelectPercentile(f_classif, percentile=percentile)\n",
    "\n",
    "    features_train_transformed_optimized = percentile_transformer.fit_transform(features_train_transformed,\n",
    "                                                                                train_df['Author']).toarray()\n",
    "    features_test_transformed_optimized = percentile_transformer.transform(features_test_transformed).toarray()\n",
    "\n",
    "    prediction = get_gauss_nb(features_train_transformed_optimized, train_df['Author'],\n",
    "                              features_test_transformed_optimized)\n",
    "    precision, recall = (precision_score(correct_labels, prediction, average='weighted'),\n",
    "                         recall_score(correct_labels, prediction, average='weighted'))\n",
    "    print(precision, recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Додаткові методи обробки тексту  (на максимальну кількість балів)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 11</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Використовуючи функцію `normalize_text` та `tokenize_text`, перетворіть колонку `Text` в навчальному та тестовому датасетах з стрічки у масив слів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "def expand_contractions(sentence: str):\n",
    "    contractions_dict = {\n",
    "        \"ain’t\": \"are not\",\n",
    "        \"’s\": \" is\",\n",
    "        \"aren’t\": \"are not\",\n",
    "        \"can’t\": \"cannot\",\n",
    "        \"can’t’ve\": \"cannot have\",\n",
    "        \"‘cause\": \"because\",\n",
    "        \"could’ve\": \"could have\",\n",
    "        \"couldn’t\": \"could not\",\n",
    "        \"couldn’t’ve\": \"could not have\",\n",
    "        \"didn’t\": \"did not\",\n",
    "        \"doesn’t\": \"does not\",\n",
    "        \"don’t\": \"do not\",\n",
    "        \"hadn’t\": \"had not\",\n",
    "        \"hadn’t’ve\": \"had not have\",\n",
    "        \"hasn’t\": \"has not\",\n",
    "        \"haven’t\": \"have not\",\n",
    "        \"he’d\": \"he would\",\n",
    "        \"he’d’ve\": \"he would have\",\n",
    "        \"he’ll\": \"he will\",\n",
    "        \"he’ll’ve\": \"he will have\",\n",
    "        \"how’d\": \"how did\",\n",
    "        \"how’d’y\": \"how do you\",\n",
    "        \"how’ll\": \"how will\",\n",
    "        \"I’d\": \"I would\",\n",
    "        \"I’d’ve\": \"I would have\",\n",
    "        \"I’ll\": \"I will\",\n",
    "        \"I’ll’ve\": \"I will have\",\n",
    "        \"I’m\": \"I am\",\n",
    "        \"I’ve\": \"I have\",\n",
    "        \"isn’t\": \"is not\",\n",
    "        \"it’d\": \"it would\",\n",
    "        \"it’d’ve\": \"it would have\",\n",
    "        \"it’ll\": \"it will\",\n",
    "        \"it’ll’ve\": \"it will have\",\n",
    "        \"let’s\": \"let us\",\n",
    "        \"ma’am\": \"madam\",\n",
    "        \"mayn’t\": \"may not\",\n",
    "        \"might’ve\": \"might have\",\n",
    "        \"mightn’t\": \"might not\",\n",
    "        \"mightn’t’ve\": \"might not have\",\n",
    "        \"must’ve\": \"must have\",\n",
    "        \"mustn’t\": \"must not\",\n",
    "        \"mustn’t’ve\": \"must not have\",\n",
    "        \"needn’t\": \"need not\",\n",
    "        \"needn’t’ve\": \"need not have\",\n",
    "        \"o’clock\": \"of the clock\",\n",
    "        \"oughtn’t\": \"ought not\",\n",
    "        \"oughtn’t’ve\": \"ought not have\",\n",
    "        \"shan’t\": \"shall not\",\n",
    "        \"sha’n’t\": \"shall not\",\n",
    "        \"shan’t’ve\": \"shall not have\",\n",
    "        \"she’d\": \"she would\",\n",
    "        \"she’d’ve\": \"she would have\",\n",
    "        \"she’ll\": \"she will\",\n",
    "        \"she’ll’ve\": \"she will have\",\n",
    "        \"should’ve\": \"should have\",\n",
    "        \"shouldn’t\": \"should not\",\n",
    "        \"shouldn’t’ve\": \"should not have\",\n",
    "        \"so’ve\": \"so have\",\n",
    "        \"that’d\": \"that would\",\n",
    "        \"that’d’ve\": \"that would have\",\n",
    "        \"there’d\": \"there would\",\n",
    "        \"there’d’ve\": \"there would have\",\n",
    "        \"they’d\": \"they would\",\n",
    "        \"they’d’ve\": \"they would have\",\n",
    "        \"they’ll\": \"they will\",\n",
    "        \"they’ll’ve\": \"they will have\",\n",
    "        \"they’re\": \"they are\",\n",
    "        \"they’ve\": \"they have\",\n",
    "        \"to’ve\": \"to have\",\n",
    "        \"wasn’t\": \"was not\",\n",
    "        \"we’d\": \"we would\",\n",
    "        \"we’d’ve\": \"we would have\",\n",
    "        \"we’ll\": \"we will\",\n",
    "        \"we’ll’ve\": \"we will have\",\n",
    "        \"we’re\": \"we are\",\n",
    "        \"we’ve\": \"we have\",\n",
    "        \"weren’t\": \"were not\",\n",
    "        \"what’ll\": \"what will\",\n",
    "        \"what’ll’ve\": \"what will have\",\n",
    "        \"what’re\": \"what are\",\n",
    "        \"what’ve\": \"what have\",\n",
    "        \"when’ve\": \"when have\",\n",
    "        \"where’d\": \"where did\",\n",
    "        \"where’ve\": \"where have\",\n",
    "        \"who’ll\": \"who will\",\n",
    "        \"who’ll’ve\": \"who will have\",\n",
    "        \"who’ve\": \"who have\",\n",
    "        \"why’ve\": \"why have\",\n",
    "        \"will’ve\": \"will have\",\n",
    "        \"won’t\": \"will not\",\n",
    "        \"won’t’ve\": \"will not have\",\n",
    "        \"would’ve\": \"would have\",\n",
    "        \"wouldn’t\": \"would not\",\n",
    "        \"wouldn’t’ve\": \"would not have\",\n",
    "        \"y’all\": \"you all\",\n",
    "        \"y’all’d\": \"you all would\",\n",
    "        \"y’all’d’ve\": \"you all would have\",\n",
    "        \"y’all’re\": \"you all are\",\n",
    "        \"y’all’ve\": \"you all have\",\n",
    "        \"you’d\": \"you would\",\n",
    "        \"you’d’ve\": \"you would have\",\n",
    "        \"you’ll\": \"you will\",\n",
    "        \"you’ll’ve\": \"you will have\",\n",
    "        \"you’re\": \"you are\",\n",
    "        \"you’ve\": \"you have\"\n",
    "    }\n",
    "\n",
    "    contractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n",
    "    return contractions_re.sub(lambda match: contractions_dict[match.group(0)], sentence)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T22:41:10.184227100Z",
     "start_time": "2023-11-20T22:41:10.165590100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:42:06.030257900Z",
     "start_time": "2023-11-20T22:41:45.095184900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0       [The, Internet, may, be, overflowing, with, new, technology, but, crime, in, cyberspace, is, still, of, the, old-fashioned, variety, ., The, Natio...\n 1       [The, U.S, ., Postal, Service, announced, Wednesday, a, plan, to, boost, online, commerce, by, enhancing, the, security, and, reliability, of, ele...\n 2       [Elementary, school, students, with, access, to, the, Internet, learned, more, than, kids, who, lacked, access, ,, an, indepedent, research, group...\n 3       [An, influential, Internet, organisation, has, backed, away, from, a, proposal, to, dramatically, expand, the, number, of, addresses, available, o...\n 4       [An, influential, Internet, organisation, has, backed, away, from, a, proposal, to, dramatically, expand, the, number, of, addresses, available, o...\n                                                                                 ...                                                                          \n 2495    [China, 's, central, bank, chief, has, said, that, inflation, would, be, a, modest, seven, percent, this, year, and, this, showed, the, state, 's,...\n 2496    [China, ushered, in, 1997, ,, a, year, it, has, hailed, as, one, of, the, most, significant, of, the, communist, era, for, the, impending, recover...\n 2497    [China, issued, tough, new, rules, on, the, handling, of, blood, products, on, Sunday, in, a, move, that, follows, the, sale, of, HIV-tainted, blo...\n 2498    [China, will, avoid, bold, moves, in, tackling, its, ailing, state, enterprises, as, it, focuses, on, stability, ahead, of, this, year, 's, crucia...\n 2499    [Communist, Party, chief, Jiang, Zemin, has, put, his, personal, stamp, on, the, legacy, of, China, 's, paramount, leader, Deng, Xiaoping, with, a...\n Name: Text, Length: 2500, dtype: object,\n 0       [U.S, ., Senators, on, Tuesday, sharply, criticized, a, new, Securities, and, Exchange, Commission, rule, forcing, companies, to, disclose, their,...\n 1       [Two, members, of, Congress, criticised, the, Federal, Reserve, Thursday, for, what, they, called, its, ``, woefully, inadequate, '', record, of, ...\n 2       [Commuters, stuck, in, traffic, on, the, Leesburg, Pike, in, Northern, Virginia, are, just, a, few, hundred, yards, away, from, an, even, bigger, ...\n 3       [A, broad, coalition, of, corporations, went, to, Capitol, Hill, on, Tuesday, to, lobby, in, favor, of, relaxed, export, restrictions, on, compute...\n 4       [On, the, Internet, ,, where, new, products, come, and, go, in, the, blink, of, an, eye, ,, time, is, said, to, move, at, ``, Internet, speed, ., ...\n                                                                                 ...                                                                          \n 2495    [China, has, scored, new, successes, in, its, fight, against, inflation, and, economists, said, on, Friday, that, price, rises, this, year, could,...\n 2496    [China, has, scored, new, successes, in, its, fight, against, inflation, and, economists, said, on, Friday, that, price, rises, this, year, could,...\n 2497    [China, is, on, target, with, plans, to, to, promote, 100, large, chemical, groups, by, 2000, by, tapping, a, $, 1.6, billion, war, chest, ,, Mini...\n 2498    [China, may, need, to, adjust, the, mix, of, its, treasury, debt, next, year, to, ensure, an, active, response, from, domestic, institutions, ,, a...\n 2499    [A, Chinese, ideologue, known, for, his, strictly, orthodox, Marxist, views, denied, on, Tuesday, that, he, was, the, author, of, two, essays, cri...\n Name: Text, Length: 2500, dtype: object)"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = load_test_data()\n",
    "train_df = load_train_data()\n",
    "\n",
    "text_train_contracted = train_df[\"Text\"].map(expand_contractions)\n",
    "text_test_contracted = test_df[\"Text\"].map(expand_contractions)\n",
    "\n",
    "train_df[\"Text\"] = text_train_contracted.map(lambda s: word_tokenize(s, 'english'))\n",
    "test_df[\"Text\"] = text_test_contracted.map(lambda s: word_tokenize(s, 'english'))\n",
    "\n",
    "train_df[\"Text\"], test_df[\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шумові слова (Stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шумові слова або стоп-слова - це слова, які не несуть смислового навантаження, тому їх користь та роль не є суттєвою. У кожної мови є свої набори слів, які практично завжди можна прибрати.\n",
    "\n",
    "Для англійської мови це наприклад:\n",
    "- артиклі (a, an, the)\n",
    "- сполучні слова (and, or)\n",
    "- займенники (I, You, We, They)\n",
    "- слова для позначення часу (is, are, be, was, were, will, have, has etc)\n",
    "- числа\n",
    "- інші частовживані слова\n",
    "\n",
    "Додатково можна видалити усі числа, окремо розташовані знаки пунктуації `. , = + /! \"; :%? * ()`.\n",
    "\n",
    "Також можна видаляти найбільш частовживані слова предметної області документів (якщо актуально)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В NLTK вже є готові побудовані набори найбільш частовживаних слів англійської мови, доступні через клас stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 12</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**a)**</span> Виведіть стоп-слова для англійської мови з пакету NLTK. Скільки всього слів?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:42:18.534713300Z",
     "start_time": "2023-11-20T22:42:18.504650200Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**b)**</span> Видаліть з масивів слів у навчальному та тестовому датафреймах стоп-слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-20T22:42:19.652373200Z"
    }
   },
   "outputs": [],
   "source": [
    "func = lambda x: [word for word in x if word not in stop_words]\n",
    "train_df['Text'] = train_df['Text'].apply(func)\n",
    "test_df['Text'] = test_df['Text'].apply(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**c)**</span> Видаліть з масивів слів у навчальному та тестовому датасетах окремі знаки пунктуації та числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-20T22:40:16.861668700Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "func = lambda x: [word for word in x if word.isalpha()]\n",
    "train_df['Text'] = train_df['Text'].apply(func)\n",
    "test_df['Text'] = test_df['Text'].apply(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Нормалізація слів"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Під початковою формою слова мається на увазі форма слова, яка не зазнала змін, необзідних для передачі граматичних категорій, таких як час, кількість, особа, стать, тощо.\n",
    "\n",
    "Наприклад, слово `бути` в залежності від часу та особи може записуватись як `був`, `була`, `були`, `є`, `буду`, `будуть`, `будете` тощо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процес приведення слова до початкової форми називаються `Лемматизацією` (Lemmatization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:40:16.861668700Z",
     "start_time": "2023-11-20T22:40:16.856612100Z"
    }
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('getting', 'v'))\n",
    "print(wnl.lemmatize('rabbits', 'n'))\n",
    "print(wnl.lemmatize('quickly', 'r'))\n",
    "print(wnl.lemmatize('slowly', 'r'))\n",
    "# KeyError! - Doesn't work on non-words!\n",
    "print(wnl.lemmatize('xyzing', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проте, лемматизації вимагає правильно визначити частину мови, що є нетривіальною задачею."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Також інколи потрібно розглядати слова не в структурі частин мови, а за коренем. Наприклад, слова `Organization` та `To Organize` з точки зору лемматизації є різними, хоча корінь в них спільний. І якщо ви часто використовуєте одне з слів, то швидше за все будете використовувати і інше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тож часто задачі аналізу тексту використовують не лемматизацію, а стеммінг (Stemming). Процес стеммінгу полягає у знаходження коренів слів (не обов'язково лінгвістично правильних)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наприклад, під час стеммінгу слова `organized`, `organization`, `organizers` можуть бути приведені до єдиного слова `organiz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK містить набір декількох стандартних стеммерів. Ознайомтесь з ними нижче.\n",
    "Ви можете використовувати також сторонні стеммери наприклад, https://pypi.org/project/stemming/1.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-20T22:12:45.675291100Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmers = [PorterStemmer(), SnowballStemmer(\"english\"), LancasterStemmer()]\n",
    "sentence = \"Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's.\"\n",
    "sentence = tokenize_text(sentence)\n",
    "print(\"Original sentence:\", ' '.join(sentence))\n",
    "for stemmer in stemmers:\n",
    "    print(type(stemmer).__name__ + \":\", ' '.join([stemmer.stem(w) for w in sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 13</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитайте детальніше документацію NLTK щодо `PorterStemmer` та `SnowballStemmer`. В чому полягає різниця між ними? Напишіть речення, для на якому вивід `PorterStemmer` та `SnowballStemmer` будуть різним."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-20T22:12:45.676846400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 14</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Використайте один із стеммерів (на ваш розсуд) для обробки слів в навчальному та тестовому датасетах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-20T22:12:45.676846400Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_words(words):\n",
    "    # Write your code here\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-20T22:12:45.676846400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['Text'] = train_df['Text'].apply(lambda x: normalize_words(x))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Завдання 15 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основі оброблених слів перебудуйте модель NaiveBayes, створену вище та проведіть її оцінку. Чи змінилась точність?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-20T22:12:45.676846400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
